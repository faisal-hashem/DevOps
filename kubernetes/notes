Accessing Kubernetes master nodes resources, you can do the following:
    curl https://kube-master:6443/version 

Accessing the Kubernetes Pods through API, do the following:
    curl https://kube-master:6443/api/v1/pods

You can do the same to access the following:
    /metrics
    /healthz
    /version
    /api
    /apis
    /logs

core api: /api
    contains namespaces, pods, rc, events, endpoints, nodes, bindings, PV, PVC, configmaps, secrets, services 

named api: /apis (going towards this for the future)
    contains: /apps - contains deployments, replicasets, statefulsets . /extensions . /networking.k8s.io - contains network policies . /storage.k8s.io . /authentication.k8s.io . /certificates.k8s.io -includes /certificatesigningrequests. etc.

can view them on your k8s cluster by the following:
    curl https://kube-master:6443 -k
    curl https://kube-master:6443/apis -k | grep 'name'

if you cannot connect, you can pass in the certificates as the following:
    curl https://kube-master:6443 -k --key user.key --cert user.crt --cacert ca.crt 

or you can launch a proxy service by runnign the following, this runs on port 8001. Uses creds from kube config file to access the cluster. 
    kubectl proxy 
    curl https://kube-master:8001 -k

Authorizations:
What can the user do after successfully logging in to the cluster. Give developers, storage/networkers different permissions. Same for service accounts. Assign minimum permissions. Retrict access to users to their namespaces alone. These are set on under /usr/local/bin/kube-apiserver --authorization-mode=AlwaysAllow. You can also set it to multiple modes, goes through each mode in the order it is specified. For example=Node, RBAC, Webhook 

Node
    By default, users and kubelet (from each node) will be allowed to access the Kube API. The node authorizer gives kubelet Read (services, endpoints, nodes, pods) and write (node status, pod status, events) permissions. Kubelet is a part of the systems nodes group, have a name prefixed with a systems node. So any requests that comes from this gets the granted these permissions by the node authorizer. 

ABAC (Attribute Based Authorization - for external users): Allowing a user/user group to have a set of permissions such as view, create and delete pods. You do this by setting a policy file with a set of policies defined in json in the api server. Difficult to manage. 

RBAC (Role based access controls): Instead of specifying permissions for each user/group, we create roles with a set permissions and assign users/groups to that role. Create a role for developers/security/datateam etc, and assign them to their dedicated roles. 

Webhook: Outsource your authorization with third party tools, example Open Policy Agent. Have k8s make an API call to the Open Policy Agent with User info and access requirements and validate if the user can access or not based on the third party tool. 

AlwaysAllow: Allows all requests without any check. Set to this by default. 
AlwaysDeny: Always denies. 


Setting up RBAC with Roles and RoleBindings:
    Check role and role binding manifest files.
    To check your access you can run: 
    kubectl auth can-i create deployments
    kubectl auth can-i delete nodes

    Run as another user to validate correct access was given:
    kubectl auth can-i create deployments -as dev-user
    kubectl auth can-i create pods -as dev-user

    Can also check namespaces. 
    kubectl auth can-i create deployments -as dev-user --namespace test

Normally roles/rolebindings are good for assigning roles to specific namespaces but for cluster roles, you can create one for creating/deleting pods and that role will be applied to ALL namespaces. Which might be usefule in certain situations. 


Cluster Resources: Nodes, PV, clusterroles, clusterrolebindings, certificatesigningrequests, namespaces (the object itself is not namespaced)

Check for namespaced resources: kubectl api-resources --namespaced=true
Check for cluster resources: kubectl api-resources --namespaced=false


Cluster Roles: Roles for cluster wide resources. Ex. Cluster Admin role is for viewing/creating/deleting Nodes. Or creating PVs. 

Cluster Role Bindings: Bind to the Cluster role. 

Kubectl --> Authentication --> Authorization --> Admission Controllers --> Create Pod

Admission Controllers: Define policies for k8s resource creations. For example, permitting images from certain registries, do not permit runAs root user, permitting certain capabilities, ensuring pods always have labels. 

Pre-built Admission Controllers: AlwaysPullImages, DefaultStorageClass, EventRateLimit, NamespaceExists etc. 

Check for Enabled Admission Controllers:
    kube-apiserver -h | grep enable-admission-plugins
OR  kubectl exec kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep enable-admission-plugins
OR ps -ef | grep kube-apiserver | grep admission-plugins

Check for not-usually enabled controllers: 

kubectl exec kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep enable-admission-plugins 

Add Admission Controllers:
    vi /etc/kubernetes/manifests/kube-apiserver.yaml

To disable Admission controller plugins, you can add the following flag: --disable-admission-plugins=DefaultStorageClass 

Mutating Admission Controllers: Can change or mutate the object itself using the admission controller. Example: DefaultStorageClass - if a storageClassName is not specified on a PVC claim then the default storage class will get added to the PVC. It MuTaTeS. 

What we worked on first is the Validating admission controller. 

Mutating admission controller runs first and then the validating admission controller runs after. This is to avoid any conflicts with the validation process. 

Create your own Mutating Admission Controller with Webhook Server:

This will authorize admission controller to send admission review requests to the webhook server and the webhook server will then respond with a response if it is allowed or not. 

- Create your own Admission Webhook Server (can be external or in the k8s cluster)
    - This can be written in GO/Python or other programming language that can accept the mutating/validating admission controller api and respond with a JSON object that the webserver expects. 
    - If hosted in the k8s cluster, you can deploy it through a deployment manifest and associate it with a service that can be accessed. 

- Configure the k8s manifest for webhook to talk to the Webhook Server
    - Create a ValidatingWebhookConfiguration OR MutatingWebhookConfiguration manifest. (see manifest folder)


API:

API Versions: 

v1alpha: Very new and contains bugs in it. Might be dropped later, so risky to use. 

v1beta: Has commitment to go to GA but may contain minor bugs. 

v1: Highly reliable and can be used. 

- In order to enable using the different API groups, you need to add it to the runtime-config=api/all or api/v2alpha1 etc.. settings in the kube-apiserver -h | grep runtime-config 

API Deprecation Policy: Mandates what version can be used and cant be used. This is on the API side, when they remove a feature they will deprecate it by updating the version. 

For example in v1alpha1 they have a feature that wasn't working. Now in v1alpha2, it has been removed. But all of our yaml manifests are pointing to v1alpha1. Now we can CHANGE our preferred/storage version to v1alpha2 and that would automatically convert all of our yaml manifests to v1alpha2. 

API Deprecation Policy 2: API objects must be able to round-trip between API versions in a given release without information loss, with the exception of whole REST resources that do not exist in some verisons. This ensures when going from v1alpha1 to v1alpha2, and back to v1alpha1, the v1alpha1 version will now contain the updated objects of v1alpha2.

API Deprecation Policy 4a: Other than the most recent API versions in each track, older API versions must be supported after their announced deprecation for a duration of no less than: GA(12 months or 3 releases), Beta(9 months or 3 releases), and Alpha(0 releases).

 API Deprecation Policy 4b: The preferred API version and the storage version for a given group may NOT advance until after a release has been made that supoorts both the new version and the previous version. Once the previous version is supported, then you can switch the preferred/storage version. 

 Once the GA version releases, the v1beta1 and v1beta2 gets deprecated after 3 releases or 9 months (whatever comes first). 

Check for the preferred version on the API server: 
kubectl proxy 8001&
curl localhost:8001/apis/authorization.k8s.io

 API Deprecation Policy 3: An API version in a given track may NOT be deprecated until a new API version at least as a stable is released. Which means a v1alpha1 version cannot deprecate a v1 version but it can only be done the other way around. For example v2 can deprecate v1. 

 In order to convert a yaml file to a different version, you can use: 
    - kubectl convert -f nginx.yaml --output-version apps/v1
    - kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1 | kubectl apply -f -


kubectl api-resources - shows all the resources with their API versions and short names and if it goes into a namespace

To change the default version for rbac.authorization.k8s.io for example. You can go to: /etc/kubernetes/manifests/kube-apiserver.yaml and add --runtime-config=rbac.authorization.k8s.io/v1alpha1. 
    - make sure to take backup of this file before making any changes as this can break your cluster. 


Custom Resource Definition: Create your own custom resource, this is just the definition file. The definitions are stored in ETCD and when you run kubectl get pod, you are grabbing it from ETCD. 

The controller is responsible for creating the actual resource definition through the worker nodes. Kubernetes is based off of GO language, so the actual code to create these resource definitions is through the GO language. Therefore, to create your own custom controller, you will need to write it out in GO language. This is continuously running and monitoring the kubernetes cluster. 

ETCD communicates with the Controller when you run commands suchs as kubectl run nginx -i=nginx:1.1 and is able to create the resources needed. 

See customresourcedef.yaml for a custom resource definition file example. Pretty much Pods, deployments, etc all has a version of this resource definition file. 

Build custom controller (using GO) (NOT on exam):
- Make sure to install GO
- git clone https://github.com/kubernetes/sample-controller.git
- cd sample-controller
- go build -o sample-controller .
- ./sample-controller -kubeconfig=$HOME/.kube/config
- Distribute it in the cluster, package it in a docker image, and run this as a pod in the cluster. 

Operator Framework (NOT on exam): Avoid deploying CRD and Controller separately. Can package them together and deploy as a single entity. 